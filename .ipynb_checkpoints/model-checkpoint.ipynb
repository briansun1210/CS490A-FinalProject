{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import scispacy\n",
    "import spacy\n",
    "import sklearn\n",
    "import warnings \n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import KFold\n",
    "# from negspacy.negation import Negex\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from collections import defaultdict\n",
    "from transformers import BertTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "# print(model1.similarity(\"Hospice\", \"Immunology\"))                               \n",
    "# nlp0 = spacy.load(\"en_core_web_sm\")\n",
    "# negex = Negex(nlp0, language = \"en_clinical\")\n",
    "# nlp0.add_pipe(negex, last=True)\n",
    "# doc0 = nlp0(\"he is not an emotional eater\")\n",
    "import string\n",
    "\n",
    "#string punctuation\n",
    "punct = set(string.punctuation)\n",
    "# for e in doc0.ents:\n",
    "# \tprint(e.text, e._.negex)\n",
    "\n",
    "# Using scispacy with \"en_core_sci_sm\":\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# # Using scispacy with \"en_core_sci_lg\":\n",
    "# nlp = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "# # Using scispacy with \"en_ner_bc5cdr_md:\n",
    "# nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "\n",
    "\n",
    "def tokenize_doc(text):\n",
    "    bow = defaultdict(float)\n",
    "    temp = []\n",
    "    doc = nlp(text)\n",
    "    doc = doc.ents\n",
    "    lowered_tokens = map(lambda t: str(t).lower(), doc)\n",
    "\n",
    "    for token in lowered_tokens:\n",
    "        # token = token.split(\".,\")\n",
    "        # if (len(token) > 1):\n",
    "        #     for j in token:\n",
    "        #         j = j.split(\":,\")\n",
    "        #         for i in j:\n",
    "        #             bow[i] += 1.0\n",
    "        #             temp.append(i)\n",
    "        # else:\n",
    "        #     for i in token:\n",
    "        #         bow[i] += 1.0\n",
    "        #         temp.append(i)\n",
    "\n",
    "        temp.append(token)\n",
    "    return temp\n",
    "\n",
    "def tokenize_doc2(text):\n",
    "    t = BertTokenizer.from_pretrained(\"bert-large-uncased-vocab.txt\", lowercase=True)\n",
    "    warnings.filterwarnings('ignore')\n",
    "    output = t.tokenize(text)\n",
    "    temp = []\n",
    "    words = [w for w in output if w not in punct]\n",
    "    for token in words:\n",
    "        temp.append(token)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mtsamples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4966\n"
     ]
    }
   ],
   "source": [
    "ids = data[\"transcription\"]\n",
    "data = data.drop_duplicates(subset = [\"description\",\"medical_specialty\",\"sample_name\",\"transcription\"])\n",
    "temp = [i for i in data.index if pd.isna(data[\"transcription\"][i])]\n",
    "data = data.drop(temp)\n",
    "print(len(data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "116\n",
      "205\n",
      "263\n",
      "459\n",
      "622\n",
      "628\n",
      "680\n",
      "729\n",
      "871\n",
      "879\n",
      "983\n",
      "984\n",
      "985\n",
      "987\n",
      "1017\n",
      "1055\n",
      "2016\n",
      "2455\n",
      "2498\n",
      "2529\n",
      "2585\n",
      "2588\n",
      "2650\n",
      "3582\n",
      "3588\n",
      "3621\n",
      "3626\n",
      "3629\n",
      "3632\n",
      "3725\n",
      "3771\n",
      "4649\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "\n",
    "for i in range(len(data[\"transcription\"])):\n",
    "    try:\n",
    "        data_list.append(data[\"transcription\"][i])\n",
    "    except:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4933\n"
     ]
    }
   ],
   "source": [
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.\n"
     ]
    }
   ],
   "source": [
    "print(data['transcription'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Model name 'bert-large-uncased-vocab.txt' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). We assumed 'bert-large-uncased-vocab.txt' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1ca3df50558d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_doc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'transcription'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-64e96e61b7aa>\u001b[0m in \u001b[0;36mtokenize_doc2\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize_doc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-large-uncased-vocab.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1583\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1584\u001b[1;33m             raise EnvironmentError(\n\u001b[0m\u001b[0;32m   1585\u001b[0m                 \u001b[1;34m\"Model name '{}' was not found in tokenizers model name list ({}). \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1586\u001b[0m                 \u001b[1;34m\"We assumed '{}' was a path, a model identifier, or url to a directory containing vocabulary files \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Model name 'bert-large-uncased-vocab.txt' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). We assumed 'bert-large-uncased-vocab.txt' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
     ]
    }
   ],
   "source": [
    "\n",
    "test = tokenize_doc2(data['transcription'][0])   \n",
    "print(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokenize_doc(data['transcription'][0])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=12, random_state=None, shuffle=False)\n",
    "kf.get_n_splits(data)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_doc)\n",
    "# Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_doc2)\n",
    "features  = Tfidf_vect.fit(data['transcription'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nsvc = NuSVC(nu=0.0001, probability=True)\n",
    "lsvc = LinearSVC(tol = 1e-5, C = 0.0001, random_state=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4e053140adc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# for train_index, test_index in kf.split(data):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kf' is not defined"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "res = {}\n",
    "for train_index, test_index in kf.split(data):\n",
    "    # print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    # for train_index, test_index in kf.split(data):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    x_train, x_test = data['transcription'][train_index], data['transcription'][test_index]\n",
    "    y_train, y_test = data['medical_specialty'][train_index], data['medical_specialty'][test_index]\n",
    "\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(x_train)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(x_test)\n",
    "    \n",
    "#     nsvc.fit(Train_X_Tfidf,y_train) \n",
    "    lsvc.fit(Train_X_Tfidf,y_train) \n",
    "    \n",
    "#     predictions_nuSVM = nsvc.predict(Test_X_Tfidf)\n",
    "    predictions_nuSVM = lsvc.predict(Test_X_Tfidf)\n",
    "    \n",
    "    print(metrics.classification_report(predictions_nuSVM, y_test.to_numpy(), digits=5))\n",
    "    total += accuracy_score(y_test.to_numpy(), predictions_nuSVM)*100\n",
    "    count += 1 \n",
    "    res[count] = (predictions_nuSVM, y_test.to_numpy())\n",
    "\n",
    "\n",
    "    print(accuracy_score(y_test, predictions_nuSVM)*100)\n",
    "#     total += accuracy_score(y_test, predictions_nuSVM)*100\n",
    "#     count += 1\n",
    "\n",
    "# print(count)\n",
    "# print(total/count)\n",
    "print(total/count)\n",
    "print(res)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tfidf_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-99d2617e201c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'medical_specialty'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'medical_specialty'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mTrain_X_Tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mTest_X_Tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tfidf_vect' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, x_test = data['transcription'][0:5], data['transcription'][0:5]\n",
    "y_train, y_test = data['medical_specialty'][0:5], data['medical_specialty'][0:5]\n",
    "                                                  \n",
    "Train_X_Tfidf = Tfidf_vect.transform(x_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(x_test)\n",
    "\n",
    "nsvc.fit(Train_X_Tfidf,y_train)   \n",
    "#classifier2.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "predictions_nuSVM = nsvc.predict(Test_X_Tfidf)\n",
    "print(accuracy_score(predictions_nuSVM, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(Tfidf_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
