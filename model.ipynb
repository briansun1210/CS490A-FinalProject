{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import scispacy\n",
    "import spacy\n",
    "import sklearn\n",
    "import warnings \n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import KFold\n",
    "# from negspacy.negation import Negex\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from collections import defaultdict\n",
    "from transformers import BertTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "# print(model1.similarity(\"Hospice\", \"Immunology\"))                               \n",
    "# nlp0 = spacy.load(\"en_core_web_sm\")\n",
    "# negex = Negex(nlp0, language = \"en_clinical\")\n",
    "# nlp0.add_pipe(negex, last=True)\n",
    "# doc0 = nlp0(\"he is not an emotional eater\")\n",
    "import string\n",
    "\n",
    "#string punctuation\n",
    "punct = set(string.punctuation)\n",
    "# for e in doc0.ents:\n",
    "# \tprint(e.text, e._.negex)\n",
    "\n",
    "# Using scispacy with \"en_core_sci_sm\":\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# # Using scispacy with \"en_core_sci_lg\":\n",
    "# nlp = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "# # Using scispacy with \"en_ner_bc5cdr_md:\n",
    "# nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "\n",
    "\n",
    "def tokenize_doc(text):\n",
    "    bow = defaultdict(float)\n",
    "    temp = []\n",
    "    doc = nlp(text)\n",
    "    doc = doc.ents\n",
    "    lowered_tokens = map(lambda t: str(t).lower(), doc)\n",
    "\n",
    "    for token in lowered_tokens:\n",
    "        # token = token.split(\".,\")\n",
    "        # if (len(token) > 1):\n",
    "        #     for j in token:\n",
    "        #         j = j.split(\":,\")\n",
    "        #         for i in j:\n",
    "        #             bow[i] += 1.0\n",
    "        #             temp.append(i)\n",
    "        # else:\n",
    "        #     for i in token:\n",
    "        #         bow[i] += 1.0\n",
    "        #         temp.append(i)\n",
    "\n",
    "        temp.append(token)\n",
    "    return temp\n",
    "\n",
    "def tokenize_doc2(text):\n",
    "    t = BertTokenizer.from_pretrained(\"bert-large-uncased-vocab.txt\", lowercase=True)\n",
    "    warnings.filterwarnings('ignore')\n",
    "    output = t.tokenize(text)\n",
    "    temp = []\n",
    "    words = [w for w in output if w not in punct]\n",
    "    for token in words:\n",
    "        temp.append(token)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mtsamples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = data[\"transcription\"]\n",
    "data = data.drop_duplicates(subset = [\"description\",\"medical_specialty\",\"sample_name\",\"transcription\"])\n",
    "temp = [i for i in data.index if pd.isna(data[\"transcription\"][i])]\n",
    "data = data.drop(temp)\n",
    "print(len(data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "for i in range(len(data[\"transcription\"])):\n",
    "    try:\n",
    "        data_list.append(data[\"transcription\"][i])\n",
    "    except:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['transcription'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test = tokenize_doc2(data['transcription'][0])   \n",
    "print(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokenize_doc(data['transcription'][0])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=12, random_state=None, shuffle=False)\n",
    "kf.get_n_splits(data)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_doc)\n",
    "# Tfidf_vect = TfidfVectorizer(tokenizer=tokenize_doc2)\n",
    "features  = Tfidf_vect.fit(data['transcription'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nsvc = NuSVC(nu=0.0001, probability=True)\n",
    "lsvc = LinearSVC(tol = 1e-5, C = 0.0001, random_state=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "res = {}\n",
    "for train_index, test_index in kf.split(data):\n",
    "    # print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    # for train_index, test_index in kf.split(data):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    x_train, x_test = data['transcription'][train_index], data['transcription'][test_index]\n",
    "    y_train, y_test = data['medical_specialty'][train_index], data['medical_specialty'][test_index]\n",
    "\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(x_train)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(x_test)\n",
    "    \n",
    "#     nsvc.fit(Train_X_Tfidf,y_train) \n",
    "    lsvc.fit(Train_X_Tfidf,y_train) \n",
    "    \n",
    "#     predictions_nuSVM = nsvc.predict(Test_X_Tfidf)\n",
    "    predictions_nuSVM = lsvc.predict(Test_X_Tfidf)\n",
    "    \n",
    "    print(metrics.classification_report(predictions_nuSVM, y_test.to_numpy(), digits=5))\n",
    "    total += accuracy_score(y_test.to_numpy(), predictions_nuSVM)*100\n",
    "    count += 1 \n",
    "    res[count] = (predictions_nuSVM, y_test.to_numpy())\n",
    "\n",
    "\n",
    "    print(accuracy_score(y_test, predictions_nuSVM)*100)\n",
    "#     total += accuracy_score(y_test, predictions_nuSVM)*100\n",
    "#     count += 1\n",
    "\n",
    "# print(count)\n",
    "# print(total/count)\n",
    "print(total/count)\n",
    "print(res)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test = data['transcription'][0:5], data['transcription'][0:5]\n",
    "y_train, y_test = data['medical_specialty'][0:5], data['medical_specialty'][0:5]\n",
    "                                                  \n",
    "Train_X_Tfidf = Tfidf_vect.transform(x_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(x_test)\n",
    "\n",
    "nsvc.fit(Train_X_Tfidf,y_train)   \n",
    "#classifier2.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "predictions_nuSVM = nsvc.predict(Test_X_Tfidf)\n",
    "print(accuracy_score(predictions_nuSVM, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(Tfidf_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
