{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f9b35e20724ecfb93f93a911c82687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import scispacy\n",
    "import spacy\n",
    "import sklearn\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['group_3' 'group_2' 'group_1']\n",
      "0       3\n",
      "1       2\n",
      "2       2\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "4961    3\n",
      "4962    3\n",
      "4963    3\n",
      "4964    3\n",
      "4965    3\n",
      "Name: medical_specialty, Length: 4966, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"mtsamples_t2.csv\")\n",
    "\n",
    "temp = [i for i in data.index if pd.isna(data[\"transcription\"][i])]\n",
    "data = data.drop(temp)\n",
    "data = data.reset_index(drop=True)\n",
    "print(data[\"medical_specialty\"].unique())\n",
    "\n",
    "d = {'group_1': 1, 'group_2': 2, 'group_3': 3}\n",
    "label = data['medical_specialty'].map(d, na_action='ignore')\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "\n",
    "\n",
    "# Loading tokenizer from the bert layer\n",
    "PRE_TRAINED_MODEL_NAME = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string  \n",
    "punct = set(string.punctuation)\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "med7 = spacy.load(\"en_core_med7_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc(text):\n",
    "    bow = defaultdict(float)\n",
    "    temp = []\n",
    "    doc = nlp(text)\n",
    "    doc = doc.ents\n",
    "    lowered_tokens = map(lambda t: str(t).lower(), doc)\n",
    "\n",
    "    for token in lowered_tokens:\n",
    "        temp.append(token)\n",
    "#     temp = getNGrams(temp,5)\n",
    "    temp = \" \".join(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniz_s = [tokenize_doc(x) for x in data['transcription']]\n",
    "# data['transcription'].apply((lambda x: tokenize_doc(x) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjective white female complaint allergies allergies lived seattle thinks claritin zyrtec short time lose effectiveness allegra summer weeks working prescription nasal sprays asthma doest daily medication flaring up.,medications medication tri-cyclen medicine allergies.,objective:,vitals weight pounds blood pressure 124/78.,heent throat erythematous exudate nasal mucosa erythematous swollen drainage tms clear.,neck supple adenopathy.,lungs clear.,assessment allergic rhinitis.,plan:,1 zyrtec allegra again loratadine prescription coverage cheaper.,2 samples nasonex nostril weeks prescription\n"
     ]
    }
   ],
   "source": [
    "print(tokeniz_s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# tokenized = pd.Series(tokenized)\n",
    "# tokenized = tokenized.apply((lambda x: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x))))\n",
    "tokenized = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x))[0:512] for x in tokeniz_s]\n",
    "\n",
    "\n",
    "# Reshaping the array:\n",
    "max_len = 0\n",
    "for i in tokenized:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tokeniz_s, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preoperative diagnoses:,1 right shoulder rotator glenohumeral rotator arthroscopy.,3 degenerative joint diagnoses:,1 right shoulder rotator glenohumeral rotator arthroscopy.,3 degenerative joint performed right shoulder hemiarthroplasty.,anesthesia general.,estimated blood loss cc.,complications none.,components depuy global shoulder system stem cemented depuy articulating head used.,brief history patient right-hand dominant female shoulder pain years affecting daily living function pain conservative treatment.,procedure patient operative suite operative field department anesthesia administered anesthetic patient beach chair position care padded right upper extremity prepped draped usual sterile fashion deltopectoral approach skin blade blunt dissection mayo scissors overlying subscapular tendon bursal tissue perforating bleeders bovie hemostasis bursa rongeur subscapular tendon rotator subacromial region evaluated rotator irreparable bone greater tuberosity articular surface biceps tendon intact diffuse synovium tendon surface surface acromion mild ware subscapular tendon bovie cautery metzenbaum scissors tied metzenbaum suture capsule repair closure capsule posterior glenoid surface evaluated cartilage contact intact humeral head evaluated ware cartilage eburnated bone central portion humeral head decision arthroplasty rotator cuff tear irreparable ware humoral head arm positioned head articular cut margin articular surface anatomic neck level articular surface intramedullary canal cancellous bone opening hand reamers size proximal inserted impacted cut grooves fins trial trial heads sampled size fit evidence motion impingement trial wound copiously irrigated suctioned dry cement cement canal level cut prosthesis inserted visualization excess cement removed care no cement left posterior joint itself _______ cement component head morris taper checked reduced.,the component motion stability position wound copiously irrigated suctioned dry capsule insertion site anterior portion attention subscapular subscapular superiorly anchored biceps tendon region anterior portion tuberosity coverage massive rotator cuff tear tissue interosseous size fiber wound copiously irrigated suctioned dry deltoid fascial split interrupted vicryl subcutaneous tissue approximated interrupted vicryl skin approximated running vicryl steri-strips adaptic abds patient sling transferred gurney department of anesthesia.,disposition patient transferred postanesthesia care unit satisfactory condition\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_input = bert_encode(X_train, tokenizer, max_len= MAX_LEN)\n",
    "# encode  test set \n",
    "test_input = bert_encode(X_test, tokenizer, max_len= MAX_LEN )\n",
    "train_labels = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import  Input\n",
    "input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"segment_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])  \n",
    "clf_output = sequence_output[:, 0, :]\n",
    "out = Dense(1, activation='sigmoid')(clf_output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 510)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 510)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 510)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 768)]        0           keras_layer[1][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            769         tf_op_layer_strided_slice_1[0][0]\n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 769\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# intilize model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 4185s 50s/step - loss: 4.0338 - accuracy: 0.0000e+00 - val_loss: 4.0630 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2\n",
    "    # epochs=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict(test_input)\n",
    "preds = test_pred.round().astype(int)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
